---
title: Language?
date: 2022-02-07
---
 > We exist in the collective story of human unconsious

 pic here

 I often ponder the teleological significance of language; the [argmax](https://en.wikipedia.org/wiki/Arg_max) over the set of all grammatical functions that maximize its utility function. Language, in its most primitive form, can be thought of as a set of strings. Strings in this sense can be thought of as a sequence of symbols from a primitive set of alphabets. The set of alphabets that once modeled in a sequential manner, following the rules of said grammar, makes up the language. Language, from my perspective, seems to be a tool “created” by humans in an attempt to compress the higher dimensional abstract realm of objective reality into subjective qualia, allowing us to transfer information from one’s perspective onto another. One could think of the brain as a sort of Turing machine-based automata. This emergence of language as a quasi-compression tool is quite pivotal.

 It is often acknowledged that there was a significant alpha in having efficient means of transferring information in the form of intelligible sounds from one person to another. It was a pivotal advantage that allowed pre-historic Homo sapiens to seemingly eradicate all other hominin species we encountered, due to the fact that we were able to organize to such a high degree given the emergence of language, albeit in its most primitive form.

 If one were to take an information theory perspective, we could start with four simple questions. How many bits of information are encoded by the transmitter over the given medium given said method? how lossily compressed is said form of communication? what is the theorized maximal bitrate, or [Shannon capacity](https://en.wikipedia.org/wiki/Channel_capacity) of said medium of information propagation? and is the entropic load minimized relative to other methods?

 To ground these in more concrete classes, we could apply this set of questions to three classes of information propagation mechanisms.

 First, and most pre-historic, as noted earlier, is unintelligible sounds and gestures as a mechanism for information propagation. Let us define an architecture for a generic information propagation system as denoted by Shannon.

- Let the *source* S, be the set containing the list of states that one can conceive as letters of a primitive alphabet.
- Let the *encoder* E, be the encoding function that transforms the written or spoken message into bits of information to be transmitted.
- Let the *channel* CH, be the medium by which said encoded signal passes.
- Let *receiver* R, be the decoding function that decompresses the encoded signal to be received by the destination.
- Let the destination D, be the entity receiving said message.


<p align="center">
    <img width="467" alt="Screen Shot 2022-08-08 at 10 12 11 AM" src="https://user-images.githubusercontent.com/73560826/194781153-bc4237f3-39af-459b-8887-86a4a6bccc98.png">
</p>

> Figure 1. A primitive information propagation system

**We define**

The amount of information produced at the source by the occurence of state ![CodeCogsEqn (5)](https://user-images.githubusercontent.com/73560826/194964206-ff4316d1-9a2c-43b5-b530-02bd2009d002.svg) :


![CodeCogsEqn (4)](https://user-images.githubusercontent.com/73560826/194963886-db6436b8-14d0-4e82-b7a9-e392b825a620.svg)


*Eq 1. Bits of information encoded at the Sender side is given by the negative logarithm of the probability distribution of the set of states*

Also, given that we know S produces a sequence of messages, we could define the entropy of said source as ![CodeCogsEqn (6)](https://user-images.githubusercontent.com/73560826/194964924-b2264ff0-86b2-40ff-8a6c-867cc911ec0b.svg)
:

<p align="center">
    <img alt="Screen Shot 2022-08-08 at 10 12 11 AM" src="https://user-images.githubusercontent.com/73560826/194964941-aa3ed967-4106-40c4-83b2-7ec83bc726b1.svg">
</p>

*Eq 2. Entropy of sender is given by the negative product of the summation of the product of the probability distribution over the set of states by the logarithm of the probability distribution over the set of states*

The same functions for bits of information encoded and entropy at source are also applied to the destination to attain those values:

<p align="center">
    <img alt="Screen Shot 2022-08-08 at 10 12 11 AM" src="https://user-images.githubusercontent.com/73560826/194965332-c5773c47-32e6-4616-8690-4f6d25929c11.svg">
</p>

*Eq 3. Bits of information encoded at the destination side is given by the negative logarithm of the probability distribution of the set of possible destination states*

Also, the entropy at the destination can be attained by ![CodeCogsEqn (9)](https://user-images.githubusercontent.com/73560826/194965394-dcdc6960-587f-4663-ae25-a54d9cdd6a36.svg)
:

<p align="center">
    <img alt="Screen Shot 2022-08-08 at 10 12 11 AM" src="https://user-images.githubusercontent.com/73560826/194965408-10ea7c75-98e0-4edf-8b27-0c41eb151f78.svg">
</p>

*Eq 4. Entropy of destination is given by the negative product of the summation of the product of the probability distribution over the set of possible destination states by the logarithm of the probability distribution over the set of destination states*

Given that we now have H(S) and H(D), we can attain the **Shannon mutual information** by the intersection of states over these two sets based on equivocation and noise.




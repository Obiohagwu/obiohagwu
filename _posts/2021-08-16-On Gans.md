---
title: "On Gans"
date: 2021-08-26
---

### What are GANS?
As you probably know, GANS are generative adverserial networks; key words - *GENERATIVE* and *ADVERSERIAL*.
It is a special class of neural network modelling as operates on the basis of adverserializing outputs between the generator and the discriminator.

** Add img here

---
### What do we mean by Generator and Discriminator?
One could think of a GAN as two models working in an adverserial manner. The generator, used/trained to generate fake data - and the discriminator, used/trained to differentiatet the fake generated images of the generator from the ground truth dataset given.
The goal of the generator is to create images that are indistinguishable from the ground truth dataset. So in that sense, we could think of the discriminator as some quasi loss function; iteratively shifting the generator to a local minimum over its loss landscpe/ latent space of outputs until an optimal output can be produced.
Both models are trying to "outsmart" each other, so as the generator get's better, the discriminator also improves. 

Depending on the complexity of the product we are trying to implement, the GANs can either be implemented by some fairly simple feed-forward network, or as complexity increases, a convolutional network, or some more complex network like a U-net.

** Add eqns here

---
### BACKGROUND (How do these little guys even work?)
Following a simple example that Ian Goodfellow (the inventor of GANs uses to explain the models), we could imagine the generator as some sort  of master forger; think Abagnale from catch me if you can. And we can imagine the discriminator as detective Hanratty, trying to catch the forger in action. The main caveat here is that we would like the detective (discriminator) to scale in detection skill as well as Abagnale (generator) scales in forging skill.

lets take a closer look into how these actually work.

#### Maximum likelihood estimation
Generally speaking not every generative model makes use or is based on maximum likelihood estimations. But, today we are lookinga at GANs, and guess what? they do make use of MLEs! 
MLEs are a subcompartment of the underlying mechanism of GANs. So what is an MLE? you could think of an MLE as a model that outputs an estimation of a probability distribution over a parameter denoted $\theta$ that maximizes the likelihood of a certain output y.
Think of it as an argmax.

we can write the MLE as:

<p align="center">
    <img src="https://user-images.githubusercontent.com/73560826/195940211-04647c3f-fa17-4917-a643-cb2d5bf21520.svg">
</p>

<p align="center">
    <img src="https://user-images.githubusercontent.com/73560826/195940428-f81b9b2e-2170-49c8-b994-fb11920eb408.svg">
</p>

By maximizing the likelihood of a model outputing the probability distribution over the parameter  , we simultaneously minimize the KL (Kullback-Leibler) divergence (kl divergence is just a quasi distance measures between probability distributions. Based on entropic load difference between distribution P and reference distribution Q. ie, a relative entropy of 0 indicated the two distributions in question have indentical quantities of information. KL divergence is just the average difference in the number of bits required to encode samples of P, using code optimized for Q rather than one optimized for P) between the data generator distribution and the model distribution, which is equivalent to maximizing the log-likelihood (loss?) of the training set.

Recall: Minimizing the KL divergence between ![CodeCogsEqn (15)](https://user-images.githubusercontent.com/73560826/195943972-f12493d3-85b1-4ae6-95b1-c503b0c99624.svg)
 and ![CodeCogsEqn (16)](https://user-images.githubusercontent.com/73560826/195943989-ed833e6f-7ce3-42f5-8686-75d3cfb68258.svg)
 is exactly equivalent to maximizing the log-loklihood of the training set
 
 <p align="center">
    <img src="https://user-images.githubusercontent.com/73560826/195944062-10c3182f-fb9e-4661-abc9-224f219c7445.svg">
</p>


### A taxonomy of deep generative models
Here is an illustration of the taxonomy of generative models. The aim is to gain a better intuition for how these function and interplay with other non-MLE based methods and other such complex models.


<p align="center">
    <img width=350, src="https://user-images.githubusercontent.com/73560826/195944577-b3b76ccd-5d50-4f96-83c6-44c5f5571025.png">
</p>

#### Variational(deterministic) vs Markov Chain apps(stochastic)

Explicit models requiring approximation, due to some caveats of intractible density functions, we have to decide between two approximation methods. We have deterministic approximation methods, which is primarily applied to variational methods, and we have stochastic methods, which are applied to Markov chain methods, as well as Monte Carlo methods.











---
title: Behind AchebeGPT
date: 2023-02-11
---

** DRAFT: STILL IN PROGRESS **

An architectural explanation and annotated version of AchebeGPT

Given that I have a slight tendency for rambling and veering from the subject matter, it would be wise to state the objective of this   

Primary task is to finetune GPT2 model pre-trained on OpenWebTxt to writings of Chinua Achebe

For blog post, we will explore a limited scope of LMs from N-gram based models to GPT2 models

- Language modeling intro from sequence modeling perspective
- Skim Encoding and Embedding techniques while also highlighting dimension reduction tools 
- Brush up on N-gram based models, RNNs, then Attention based model
- Expand on the Fine-tuning on achebe openwebtext dataset
- Pre-training requirements (hardware & time)